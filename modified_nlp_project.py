# -*- coding: utf-8 -*-
"""Modified_nlp_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eJs-p6VlfVaE1kjQCC4cZEzev4la8Quo
"""

!pip install pandas numpy scikit-learn nltk transformers datasets beautifulsoup4 seaborn matplotlib wordcloud xgboost lxml
!pip uninstall -y torch torchvision torchaudio transformers
!pip install torch torchvision torchaudio transformers --upgrade

import pandas as pd
import numpy as np
import re
import nltk
import torch
import joblib
import seaborn as sns
import matplotlib.pyplot as plt
import requests
from bs4 import BeautifulSoup
from wordcloud import WordCloud
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from xgboost import XGBClassifier
from datasets import load_dataset
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments

nltk.download("stopwords")
from nltk.corpus import stopwords

dataset = load_dataset("reubenjohn/stackoverflow-unified-text-open-status-classification-sample")
df = dataset["train"].to_pandas()
df = df.sample(frac=0.1, random_state=42)  # Take 10% sample for speed
print(df.shape)
print(df.head())

dataset = load_dataset("reubenjohn/stackoverflow-unified-text-open-status-classification-sample")
df = dataset["train"].to_pandas()
df = df.sample(frac=0.1, random_state=42)  # Take 10% sample for speed
print(df.shape)
print(df.head())

import warnings
warnings.filterwarnings("ignore", category=UserWarning)

def clean_text(text):
    """Remove HTML tags, convert to lowercase, and remove special characters."""
    if not isinstance(text, str):
        return ""

    text = BeautifulSoup(text, "lxml").get_text()  # Use lxml to parse XML/HTML
    text = text.lower()
    text = re.sub(r'\s+', ' ', text)  # Remove extra spaces
    text = re.sub(r'[^a-zA-Z0-9]', ' ', text)  # Remove special characters
    return text.strip()

# Apply text cleaning
df["unified_texts"] = (df["Title"].fillna("") + " " + df["BodyMarkdown"].fillna("")).apply(clean_text)

# Map OpenStatus to numerical values
df["OpenStatus_id"] = df["OpenStatus"].factorize()[0]

print(df.head())

import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

# Set style
sns.set(style="whitegrid")

# Create a 3-row, 2-column layout
fig, axes = plt.subplots(3, 2, figsize=(14, 12))  # Increase figure size for better spacing

# Adjust spacing between plots (hspace: vertical space, wspace: horizontal space)
plt.subplots_adjust(hspace=0.4, wspace=0.3)

# --- 1. Distribution of OpenStatus ---
sns.countplot(ax=axes[0, 0], x=df["OpenStatus"], palette="coolwarm", order=df["OpenStatus"].value_counts().index)
axes[0, 0].set_title("Distribution of OpenStatus", fontsize=14)
axes[0, 0].set_xlabel("OpenStatus", fontsize=12)
axes[0, 0].set_ylabel("Count", fontsize=12)
axes[0, 0].tick_params(axis='x', rotation=30)

# --- 2. Pie Chart for Class Distribution ---
df["OpenStatus"].value_counts().plot.pie(
    ax=axes[0, 1], autopct="%1.1f%%", cmap="coolwarm", startangle=90, shadow=True
)
axes[0, 1].set_title("Proportion of Each OpenStatus Category", fontsize=14)
axes[0, 1].set_ylabel("")  # Hide y-axis label

# --- 3. Word Count Distribution ---
df["word_count"] = df["Title"].apply(lambda x: len(str(x).split()))
sns.boxplot(ax=axes[1, 0], x="OpenStatus", y="word_count", data=df, palette="coolwarm")
axes[1, 0].set_title("Word Count Distribution Across OpenStatus Categories", fontsize=14)
axes[1, 0].set_xlabel("OpenStatus", fontsize=12)
axes[1, 0].set_ylabel("Word Count", fontsize=12)
axes[1, 0].tick_params(axis='x', rotation=30)

# --- 4. Character Count Distribution ---
df["char_count"] = df["Title"].apply(lambda x: len(str(x)))
sns.boxplot(ax=axes[1, 1], x="OpenStatus", y="char_count", data=df, palette="coolwarm")
axes[1, 1].set_title("Character Count Distribution Across OpenStatus Categories", fontsize=14)
axes[1, 1].set_xlabel("OpenStatus", fontsize=12)
axes[1, 1].set_ylabel("Character Count", fontsize=12)
axes[1, 1].tick_params(axis='x', rotation=30)

# --- 5. Word Cloud (Plotted Separately) ---
text = " ".join(df["Title"].astype(str) + " " + df["BodyMarkdown"].astype(str))
wordcloud = WordCloud(width=1000, height=500, background_color="black", colormap="viridis").generate(text)

plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.title("Most Common Words in Questions", fontsize=14)

# Final layout adjustment
plt.tight_layout()
plt.show()

# Extract train, test, and validation sets
train_df = dataset["train"].to_pandas()
test_df = dataset["test"].to_pandas()
valid_df = dataset["valid"].to_pandas()

# Function to preprocess text
def clean_text(text):
    """Remove HTML tags, convert to lowercase, and remove special characters."""
    if not isinstance(text, str):
        return ""

    text = BeautifulSoup(text, "lxml").get_text()  # Use lxml to parse XML/HTML
    text = text.lower()
    text = re.sub(r'\s+', ' ', text)  # Remove extra spaces
    text = re.sub(r'[^a-zA-Z0-9]', ' ', text)  # Remove special characters
    return text.strip()

# Apply text cleaning
train_df["unified_texts"] = (train_df["Title"].fillna("") + " " + train_df["BodyMarkdown"].fillna("")).apply(clean_text)
test_df["unified_texts"] = (test_df["Title"].fillna("") + " " + test_df["BodyMarkdown"].fillna("")).apply(clean_text)
valid_df["unified_texts"] = (valid_df["Title"].fillna("") + " " + valid_df["BodyMarkdown"].fillna("")).apply(clean_text)

# Convert OpenStatus to numerical values
train_df["OpenStatus_id"] = train_df["OpenStatus"].factorize()[0]
test_df["OpenStatus_id"] = test_df["OpenStatus"].map(train_df[["OpenStatus", "OpenStatus_id"]].drop_duplicates().set_index("OpenStatus")["OpenStatus_id"])
valid_df["OpenStatus_id"] = valid_df["OpenStatus"].map(train_df[["OpenStatus", "OpenStatus_id"]].drop_duplicates().set_index("OpenStatus")["OpenStatus_id"])

# TF-IDF Vectorization
tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words="english")
X_train_tfidf = tfidf_vectorizer.fit_transform(train_df["unified_texts"])
X_test_tfidf = tfidf_vectorizer.transform(test_df["unified_texts"])
X_valid_tfidf = tfidf_vectorizer.transform(valid_df["unified_texts"])

# Metadata Features
train_meta = train_df[["ReputationAtPostCreation", "OwnerUndeletedAnswerCountAtPostTime"]].fillna(0)
test_meta = test_df[["ReputationAtPostCreation", "OwnerUndeletedAnswerCountAtPostTime"]].fillna(0)
valid_meta = valid_df[["ReputationAtPostCreation", "OwnerUndeletedAnswerCountAtPostTime"]].fillna(0)

# Combine Features
X_train = np.hstack((X_train_tfidf.toarray(), train_meta.to_numpy()))
X_test = np.hstack((X_test_tfidf.toarray(), test_meta.to_numpy()))
X_valid = np.hstack((X_valid_tfidf.toarray(), valid_meta.to_numpy()))
y_train = train_df["OpenStatus_id"]
y_test = test_df["OpenStatus_id"]
y_valid = valid_df["OpenStatus_id"]

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc
from sklearn.preprocessing import label_binarize

# Convert NumPy arrays to Pandas for sampling
X_train_df = pd.DataFrame(X_train)
y_train_df = pd.Series(y_train)

# Sample only 10% of the data for faster training
X_train_small = X_train_df.sample(frac=0.1, random_state=42).values  # Convert back to NumPy
y_train_small = y_train_df.sample(frac=0.1, random_state=42).values  # Convert back to NumPy

# Binarize the target labels for multi-class ROC
y_test_bin = label_binarize(y_test, classes=np.unique(y_test))
n_classes = y_test_bin.shape[1]

# Initialize XGBoost Classifier with optimized parameters
xgb_clf = XGBClassifier(n_estimators=50, learning_rate=0.05, max_depth=3, verbosity=1)

# Train XGBoost only once (no loop!)
xgb_clf.fit(X_train_small, y_train_small, eval_set=[(X_test, y_test)], verbose=False)

# Predict on Test Data
y_pred_xgb_test = xgb_clf.predict(X_test)
y_pred_xgb_proba = xgb_clf.predict_proba(X_test)  # Probabilities for ROC-AUC

# Evaluate Model
print("\nXGBoost Test Accuracy:", accuracy_score(y_test, y_pred_xgb_test))
print(classification_report(y_test, y_pred_xgb_test))

# ------------------- Plot ROC Curve -------------------
plt.figure(figsize=(10, 6))
for i in range(n_classes):
    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_xgb_proba[:, i])
    plt.plot(fpr, tpr, label=f'Class {i} (AUC={auc(fpr, tpr):.2f})')

plt.plot([0, 1], [0, 1], 'k--')  # Diagonal reference line
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("XGBoost ROC Curve")
plt.legend()
plt.grid()
plt.show()

# Load BERT Tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# Tokenize Text
def tokenize_function(example):
    return tokenizer(example["unified_texts"], truncation=True, padding="max_length")

dataset = dataset.map(tokenize_function, batched=True)

# Load BERT Model
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=len(df["OpenStatus_id"].unique()))

# Training Arguments
training_args = TrainingArguments(output_dir="./results", evaluation_strategy="epoch")

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
)

# Train BERT Model
trainer.train()

def predict_question_status(question_title, question_body):
    """Predicts the OpenStatus of a given Stack Overflow question."""
    text = clean_text(question_title + " " + question_body)

    # Convert text to TF-IDF vector
    text_tfidf = tfidf_vectorizer.transform([text])

    # Use average metadata values
    avg_metadata = np.array([[1000, 1]])
    text_combined = np.hstack((text_tfidf.toarray(), avg_metadata))

    # Predict using best model (XGBoost)
    predicted_status = xgb_clf.predict(text_combined)[0]

    # if predicted_status == 0:
    #     predicted_status = "open"
    # elif predicted_status == 1:
    #     predicted_status = "not a real question"
    # elif predicted_status == 2:
    #     predicted_status = "off topic"
    # elif predicted_status == 3:
    #   predicted_status = "too localized"
    # elif predicted_status == 4:
    #   predicted_status = "not contructive"
    print(f"Predicted Status: {predicted_status}")

# Example Prediction
question_title = "how to change the general JAVA COFFEE CUP icon in JAR file"
question_body = "I can change the JAVA COFFEE CUP icon in my Frame (up,left), but how could I change the RUNNABLE JAR file's general JAVA COFFEE CUP picture? Is it possible at all, to change only one JAR file's file icon? Thx"
predict_question_status(question_title, question_body)